{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Ensemble Techniques"
      ],
      "metadata": {
        "id": "VITCOm_Qt4g7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "\n",
        "   Ensemble Learning is a technique in machine learning where we combine multiple models (called “base learners” or “weak learners”) to build a more accurate and robust final model.\n",
        "\n",
        "Common Ensemble Methods:\n",
        "\n",
        "   Bagging (Bootstrap Aggregating)\n",
        "\n",
        "   Boosting\n",
        "\n",
        "   Stacking\n",
        "\n",
        "2. What is the difference between Bagging and Boosting?\n",
        "\n",
        "   **Bagging (Bootstrap Aggregating):**\n",
        "\n",
        "   Trains multiple models independently and in parallel on different random subsets of the training data.\n",
        "\n",
        "   Uses sampling with replacement to create these subsets.\n",
        "\n",
        "   The main goal is to reduce variance and prevent overfitting.\n",
        "\n",
        "   Final prediction is made by averaging (for regression) or majority voting (for classification).\n",
        "\n",
        "   Example algorithm: Random Forest.\n",
        "\n",
        "\n",
        "   **Boosting:**\n",
        "\n",
        "   Trains models sequentially, where each new model tries to fix the errors made by the previous models.\n",
        "\n",
        "   Uses the entire dataset, but gives more weight to misclassified samples after each round.\n",
        "\n",
        "   The main goal is to reduce bias and improve accuracy.\n",
        "\n",
        "   Final prediction is made by combining models using a weighted sum, giving more importance to better models.\n",
        "\n",
        "  Example algorithms: AdaBoost, Gradient Boosting, XGBoost.\n",
        "\n",
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "   \n",
        "   **Bootstrap Sampling:**\n",
        "\n",
        "   Bootstrap sampling is a technique where we create multiple random samples from the original dataset by sampling with replacement.\n",
        "\n",
        "   **Role in Bagging:**\n",
        "\n",
        "   Each model (like each decision tree in a Random Forest) is trained on a different bootstrap sample of the data.\n",
        "\n",
        "   Because every model sees a slightly different dataset, they learn different patterns and make different errors.\n",
        "\n",
        "   When we combine all the models’ predictions (by averaging or majority vote), the overall result becomes more stable and accurate.\n",
        "\n",
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "\n",
        "   **Out-of-Bag (OOB) Samples: **\n",
        "\n",
        "   In Bagging (like in Random Forest), each model is trained on a bootstrap sample — a random sample taken with replacement from the original dataset.\n",
        "   Because of this, about 63% of the data points are used in training each model, and the remaining 37% of the data points are not included in that sample.\n",
        "\n",
        "   **OOB Score:**\n",
        "\n",
        "      The OOB Score is the average prediction accuracy (or error) calculated using all OOB samples across all models in the ensemble.\n",
        "\n",
        "      It provides an unbiased estimate of the model’s performance — similar to what you’d get from cross-validation.\n",
        "\n",
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "\n",
        "   **In a Single Decision Tree:**\n",
        "\n",
        "      Feature importance is based on how much a feature reduces impurity (like Gini impurity or entropy) when it’s used to split the data.\n",
        "\n",
        "      The more a feature helps in correctly classifying data (i.e., reduces impurity), the higher its importance score.\n",
        "\n",
        "      Since the tree is built on one dataset, the importance values may be biased or unstable — small data changes can lead to big differences in importance.\n",
        "\n",
        "\n",
        "   **In a Random Forest:**\n",
        "\n",
        "   A Random Forest builds many decision trees on different bootstrap samples of data and averages their results.\n",
        "\n",
        "   Feature importance is calculated by averaging the importance of each feature across all trees.\n",
        "\n",
        "   This process gives a more stable, accurate, and unbiased estimate of which features are truly important.\n",
        "\n",
        "   It reduces the effect of randomness or overfitting that might happen in a single tree."
      ],
      "metadata": {
        "id": "NGpaLbSTt2pJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Print the top 5 most important features based on feature importance scores.\n"
      ],
      "metadata": {
        "id": "w6x-LYk7xhGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "model = RandomForestClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "feature_importance = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
        "feature_importance = feature_importance.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(feature_importance.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSi2QrlzxgWa",
        "outputId": "f4c3644e-c0f4-4f17-cb95-3ad6822a2bf2"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.  Write a Python program to:\n",
        "\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "jGVkYc6RxwKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "dt_pred = dt.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "bag = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag.fit(X_train, y_train)\n",
        "bag_pred = bag.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "print(\"Accuracy of Single Decision Tree:\", dt_acc)\n",
        "print(\"Accuracy of Bagging Classifier:\", bag_acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7_u-2d9yLBS",
        "outputId": "d74c169f-0a7f-45d3-c64c-5770733b4352"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 1.0\n",
            "Accuracy of Bagging Classifier: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "\n",
        "● Train a Random Forest Classifier\n",
        "\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "zCMxISo1yePo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],\n",
        "    'max_depth': [2, 4, 6, None]\n",
        "}\n",
        "\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Final Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK2DCKi5yqFo",
        "outputId": "6977dd9d-928d-42a7-c2c8-ebb09e5ef89e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 2, 'n_estimators': 150}\n",
            "Final Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "● Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "8XygjRfZyd1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "bag_pred = bag_reg.predict(X_test)\n",
        "bag_mse = mean_squared_error(y_test, bag_pred)\n",
        "\n",
        "rf_reg = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "rf_pred = rf_reg.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"Mean Squared Error (Bagging Regressor):\", bag_mse)\n",
        "print(\"Mean Squared Error (Random Forest Regressor):\", rf_mse)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETQSIkRozDFI",
        "outputId": "614d4f3f-53e6-4d65-a88a-ed9803f46ec0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.25787382250585034\n",
            "Mean Squared Error (Random Forest Regressor): 0.25772464361712627\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " 10. You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "● Choose between Bagging or Boosting\n",
        "\n",
        "● Handle overfitting\n",
        "\n",
        "● Select base models\n",
        "\n",
        "● Evaluate performance using cross-validation\n",
        "\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "   **Answer**\n",
        "\n",
        "   **1. Choosing between Bagging and Boosting:**\n",
        "\n",
        "If the model is overfitting or data is noisy, use Bagging (e.g., Random Forest) to reduce variance and make the model stable.\n",
        "\n",
        "If the model is underfitting and we need higher accuracy, use Boosting (e.g., XGBoost, AdaBoost) to reduce bias and improve performance.\n",
        "\n",
        "**2. Handling Overfitting:**\n",
        "\n",
        "Use cross-validation and regularization parameters like max_depth, min_samples_leaf, etc.\n",
        "\n",
        "In Boosting, use a small learning rate and early stopping.\n",
        "\n",
        "Limit the number of trees (n_estimators) and remove irrelevant features.\n",
        "\n",
        "**3. Selecting Base Models:**\n",
        "\n",
        "Use Decision Trees as base learners for both Bagging and Boosting.\n",
        "\n",
        "Try multiple models like Random Forest, XGBoost, or CatBoost and choose the one giving best validation accuracy.\n",
        "\n",
        "**4. Evaluating Performance using Cross-Validation:**\n",
        "\n",
        "Use Stratified K-Fold Cross-Validation to keep the class balance of defaulters and non-defaulters.\n",
        "\n",
        "Measure performance using Accuracy, Precision, Recall, F1-Score, and AUC-ROC.\n",
        "\n",
        "\n",
        "**5. How Ensemble Learning Improves Decision-Making:**\n",
        "\n",
        "Combines multiple models → gives more accurate and stable predictions.\n",
        "\n",
        "Reduces bias and variance, leading to better generalization on new customers.\n",
        "\n",
        "Helps financial institutions detect high-risk customers more reliably, reducing loan losses.\n",
        "\n"
      ],
      "metadata": {
        "id": "9omyQtOSydUA"
      }
    }
  ]
}